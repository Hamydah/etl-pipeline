{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Final Project ETL Notebook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!pip install -q openpyxl schedule"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os, sqlite3, random, json, pandas as pd, numpy as np, schedule, time\n",
        "from datetime import datetime, timedelta\n",
        "os.makedirs('data', exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Generate six data sources\n",
        "dates = pd.date_range('2023-01-01', datetime.today().strftime('%Y-%m-%d'))\n",
        "tickers = ['AAPL','GOOGL','MSFT','AMZN','TSLA']\n",
        "rows = []\n",
        "for t in tickers:\n",
        "    for d in dates:\n",
        "        rows.append({\n",
        "            'Date': d.strftime('%Y-%m-%d'),\n",
        "            'Ticker': t,\n",
        "            'Open': round(random.uniform(100,400),2),\n",
        "            'Close': round(random.uniform(100,400),2),\n",
        "            'Volume': random.randint(1000000,5000000)\n",
        "        })\n",
        "df_csv_1 = pd.DataFrame(rows)\n",
        "df_csv_1.to_csv('data/financial_data.csv', index=False)\n",
        "\n",
        "news_items = []\n",
        "base = datetime.utcnow()\n",
        "for i in range(1,21):\n",
        "    news_items.append({\n",
        "        'title': f'Headline {i}',\n",
        "        'desc': f'Description {i}',\n",
        "        'published': (base - timedelta(hours=i)).strftime('%Y-%m-%dT%H:%M:%SZ')\n",
        "    })\n",
        "with open('data/financial_news.json','w') as f:\n",
        "    json.dump({'news': news_items}, f)\n",
        "\n",
        "port_rows = []\n",
        "accts = ['Brokerage','Retirement','Savings']\n",
        "syms = tickers + ['FB','NVDA','NFLX']\n",
        "for _ in range(50):\n",
        "    port_rows.append({\n",
        "        'Account': random.choice(accts),\n",
        "        'Symbol': random.choice(syms),\n",
        "        'Shares': random.randint(1,500),\n",
        "        'Purchase_Price': round(random.uniform(50,3000),2)\n",
        "    })\n",
        "pd.DataFrame(port_rows).to_excel('data/portfolio_data.xlsx', index=False)\n",
        "\n",
        "conn = sqlite3.connect('data/financial_db.sqlite')\n",
        "c = conn.cursor()\n",
        "c.execute(\"DROP TABLE IF EXISTS transactions\")\n",
        "c.execute(\"CREATE TABLE transactions (id INTEGER PRIMARY KEY, ticker TEXT, txn_type TEXT, qty INTEGER, price REAL, txn_date TEXT)\")\n",
        "txns = []\n",
        "for _ in range(20):\n",
        "    date_ = (datetime.today() - timedelta(days=random.randint(1,365))).strftime('%Y-%m-%d')\n",
        "    txns.append((random.choice(syms), random.choice(['BUY','SELL']), random.randint(1,100), round(random.uniform(50,3000),2), date_))\n",
        "c.executemany(\"INSERT INTO transactions (ticker,txn_type,qty,price,txn_date) VALUES (?,?,?,?,?)\", txns)\n",
        "conn.commit()\n",
        "conn.close()\n",
        "\n",
        "rates = {\n",
        "    'USD_EUR': round(random.uniform(0.8,1.0),4),\n",
        "    'USD_GBP': round(random.uniform(0.7,0.9),4),\n",
        "    'USD_JPY': round(random.uniform(90,140),2)\n",
        "}\n",
        "with open('data/exchange_rates.json','w') as f:\n",
        "    json.dump({'exchange_rates': rates, 'time': datetime.utcnow().strftime('%Y-%m-%dT%H:%M:%SZ')}, f)\n",
        "\n",
        "api_items = []\n",
        "for i in range(1,16):\n",
        "    api_items.append({\n",
        "        'info': f'API Data {i}',\n",
        "        'value': random.randint(100,999)\n",
        "    })\n",
        "with open('data/live_api_data.json','w') as f:\n",
        "    json.dump({'live': api_items}, f)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df_csv_1_ex = pd.read_csv('data/financial_data.csv')\n",
        "df_json_1_ex = pd.DataFrame(json.load(open('data/financial_news.json'))['news'])\n",
        "df_xlsx_ex = pd.read_excel('data/portfolio_data.xlsx')\n",
        "conn = sqlite3.connect('data/financial_db.sqlite')\n",
        "df_sql_ex = pd.read_sql('SELECT * FROM transactions', conn)\n",
        "conn.close()\n",
        "df_json_2_ex = pd.DataFrame([json.load(open('data/exchange_rates.json'))['exchange_rates']])\n",
        "df_api_ex = pd.DataFrame(json.load(open('data/live_api_data.json'))['live'])\n",
        "print('Extraction complete.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def transform_generic(df):\n",
        "    df = df.drop_duplicates().dropna()\n",
        "    for c in df.columns:\n",
        "        if 'date' in c.lower() or 'time' in c.lower() or 'published' in c.lower():\n",
        "            df[c] = pd.to_datetime(df[c], errors='coerce').dt.strftime('%Y-%m-%dT%H:%M:%SZ')\n",
        "    return df\n",
        "\n",
        "df_csv_1_tr = transform_generic(df_csv_1_ex)\n",
        "df_json_1_tr = transform_generic(df_json_1_ex)\n",
        "df_xlsx_tr = transform_generic(df_xlsx_ex)\n",
        "df_sql_tr = transform_generic(df_sql_ex)\n",
        "df_json_2_tr = transform_generic(df_json_2_ex)\n",
        "df_api_tr = transform_generic(df_api_ex)\n",
        "print('Transformation complete.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "conn = sqlite3.connect('data/final_db.sqlite')\n",
        "df_csv_1_tr.to_sql('cleaned_financial_data', conn, if_exists='replace', index=False)\n",
        "df_json_1_tr.to_sql('cleaned_financial_news', conn, if_exists='replace', index=False)\n",
        "df_xlsx_tr.to_sql('cleaned_portfolio_data', conn, if_exists='replace', index=False)\n",
        "df_sql_tr.to_sql('cleaned_transactions', conn, if_exists='replace', index=False)\n",
        "df_json_2_tr.to_sql('cleaned_exchange_rates', conn, if_exists='replace', index=False)\n",
        "df_api_tr.to_sql('cleaned_api_data', conn, if_exists='replace', index=False)\n",
        "conn.close()\n",
        "print('Loading complete.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for name, df in [\n",
        "    ('financial_data.csv', df_csv_1_tr),\n",
        "    ('financial_news.json', df_json_1_tr),\n",
        "    ('portfolio_data.xlsx', df_xlsx_tr),\n",
        "    ('transactions', df_sql_tr),\n",
        "    ('exchange_rates.json', df_json_2_tr),\n",
        "    ('live_api_data.json', df_api_tr)\n",
        "]:\n",
        "    print(name, len(df))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def run_etl():\n",
        "    import sqlite3, pandas as pd, json\n",
        "    from datetime import datetime\n",
        "    print(str(datetime.now()), 'ETL started.')\n",
        "    # Extraction\n",
        "    df1 = pd.read_csv('data/financial_data.csv')\n",
        "    df2 = pd.DataFrame(json.load(open('data/financial_news.json'))['news'])\n",
        "    df3 = pd.read_excel('data/portfolio_data.xlsx')\n",
        "    conn = sqlite3.connect('data/financial_db.sqlite')\n",
        "    df4 = pd.read_sql('SELECT * FROM transactions', conn)\n",
        "    conn.close()\n",
        "    dfx = json.load(open('data/exchange_rates.json'))\n",
        "    df5 = pd.DataFrame([dfx['exchange_rates']])\n",
        "    df6 = pd.DataFrame(json.load(open('data/live_api_data.json'))['live'])\n",
        "\n",
        "    def transform_generic(df):\n",
        "        df = df.drop_duplicates().dropna()\n",
        "        for c in df.columns:\n",
        "            if 'date' in c.lower() or 'time' in c.lower() or 'published' in c.lower():\n",
        "                df[c] = pd.to_datetime(df[c], errors='coerce').dt.strftime('%Y-%m-%dT%H:%M:%SZ')\n",
        "        return df\n",
        "\n",
        "    df1t = transform_generic(df1)\n",
        "    df2t = transform_generic(df2)\n",
        "    df3t = transform_generic(df3)\n",
        "    df4t = transform_generic(df4)\n",
        "    df5t = transform_generic(df5)\n",
        "    df6t = transform_generic(df6)\n",
        "\n",
        "    conn2 = sqlite3.connect('data/final_db.sqlite')\n",
        "    df1t.to_sql('cleaned_financial_data', conn2, if_exists='replace', index=False)\n",
        "    df2t.to_sql('cleaned_financial_news', conn2, if_exists='replace', index=False)\n",
        "    df3t.to_sql('cleaned_portfolio_data', conn2, if_exists='replace', index=False)\n",
        "    df4t.to_sql('cleaned_transactions', conn2, if_exists='replace', index=False)\n",
        "    df5t.to_sql('cleaned_exchange_rates', conn2, if_exists='replace', index=False)\n",
        "    df6t.to_sql('cleaned_api_data', conn2, if_exists='replace', index=False)\n",
        "    conn2.close()\n",
        "    print(str(datetime.now()), 'ETL finished.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Automation example with schedule\n",
        "def job():\n",
        "    run_etl()\n",
        "\n",
        "schedule.every().day.at(\"02:00\").do(job)\n",
        "print(\"Scheduler set to run ETL at 02:00 daily.\")\n",
        "# Uncomment to actually run scheduler in a blocking loop.\n",
        "# while True:\n",
        "#     schedule.run_pending()\n",
        "#     time.sleep(60)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```yaml\n",
        "name: ETL Pipeline CI\n",
        "on: [push, pull_request]\n",
        "jobs:\n",
        "  etl:\n",
        "    runs-on: ubuntu-latest\n",
        "    steps:\n",
        "      - name: Checkout\n",
        "        uses: actions/checkout@v2\n",
        "      - name: Set up Python\n",
        "        uses: actions/setup-python@v2\n",
        "        with:\n",
        "          python-version: \"3.9\"\n",
        "      - name: Install dependencies\n",
        "        run: |\n",
        "          pip install -r requirements.txt\n",
        "      - name: Run ETL\n",
        "        run: python etl_pipeline.py\n",
        "      - name: Validate\n",
        "        run: |\n",
        "          python - <<EOF\n",
        "          import sqlite3, sys\n",
        "          conn = sqlite3.connect(\"data/final_db.sqlite\")\n",
        "          tables = [\"cleaned_financial_data\",\"cleaned_financial_news\",\"cleaned_portfolio_data\",\"cleaned_transactions\",\"cleaned_exchange_rates\",\"cleaned_api_data\"]\n",
        "          for t in tables:\n",
        "              c = conn.execute(f\"SELECT COUNT(*) FROM {t}\").fetchone()[0]\n",
        "              print(t, c)\n",
        "              if c == 0:\n",
        "                  sys.exit(1)\n",
        "          conn.close()\n",
        "          EOF\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Final upload to a \"production\" DB\n",
        "# Using a second SQLite here without instructions.\n",
        "def final_upload():\n",
        "    import sqlite3, pandas as pd\n",
        "    conn = sqlite3.connect('data/final_db.sqlite')\n",
        "    df_fin = pd.read_sql('SELECT * FROM cleaned_financial_data', conn)\n",
        "    conn.close()\n",
        "    conn2 = sqlite3.connect('data/production_db.sqlite')\n",
        "    df_fin.to_sql('prod_financial_data', conn2, if_exists='replace', index=False)\n",
        "    conn2.close()\n",
        "final_upload()\n",
        "print('Uploaded to production_db.sqlite.')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
